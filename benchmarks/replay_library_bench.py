#!/usr/bin/env python3
"""Deterministic replay harness for library benchmark reports.

Given a benchmark report generated by benchmarks/run_library_bench.py, this harness:
1. Reconstructs the benchmark plan from the report.
2. Verifies protocol file hashes recorded in the report.
3. Replays the benchmark with the same config over the same ordered protocol set.
4. Compares canonical deterministic projections and emits a machine-readable diff report.
"""

from __future__ import annotations

import argparse
import copy
import hashlib
import json
import subprocess
import sys
import tempfile
from pathlib import Path
from typing import Any

from run_library_bench import compute_replay_plan_sha256, extract_run_replay_projection


ROOT = Path(__file__).resolve().parent.parent


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Replay a benchmark report and check deterministic functional equivalence."
    )
    parser.add_argument("--report", required=True, help="Input benchmark report JSON.")
    parser.add_argument(
        "--out-report",
        default="",
        help="Path to write replay report JSON. Default: temp file under /tmp.",
    )
    parser.add_argument(
        "--out-comparison",
        default="",
        help="Path to write comparison JSON. Default: <out-report>.comparison.json",
    )
    parser.add_argument(
        "--skip-build",
        action="store_true",
        help="Pass --skip-build to run_library_bench.py for replay.",
    )
    parser.add_argument(
        "--max-protocols",
        type=int,
        default=0,
        help="Optional cap on replayed protocols (deterministic prefix order). 0 means all.",
    )
    parser.add_argument(
        "--allow-env-mismatch",
        action="store_true",
        help="Allow replay even if benchmark environment metadata differs.",
    )
    return parser.parse_args()


def load_json(path: Path) -> dict[str, Any]:
    obj = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(obj, dict):
        raise ValueError(f"expected JSON object in {path}")
    return obj


def sha256_file(path: Path) -> str:
    digest = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            digest.update(chunk)
    return digest.hexdigest()


def validate_report(report: dict[str, Any]) -> list[str]:
    errors: list[str] = []
    if report.get("schema_version") != 1:
        errors.append("schema_version must be 1")

    config = report.get("config")
    if not isinstance(config, dict):
        errors.append("missing config object")
    else:
        required = [
            "mode",
            "solver",
            "depth",
            "k",
            "timeout_secs",
            "samples",
            "soundness",
            "fairness",
            "require_pass",
            "require_expectations",
        ]
        for key in required:
            if key not in config:
                errors.append(f"config missing '{key}'")

    runs = report.get("runs")
    if not isinstance(runs, list) or not runs:
        errors.append("runs must be a non-empty array")
    else:
        for idx, run in enumerate(runs):
            if not isinstance(run, dict):
                errors.append(f"runs[{idx}] must be object")
                continue
            for key in ("protocol", "overall", "ok", "run_is_valid"):
                if key not in run:
                    errors.append(f"runs[{idx}] missing '{key}'")
            if "protocol_sha256" not in run:
                errors.append(
                    f"runs[{idx}] missing 'protocol_sha256' (required for deterministic replay)"
                )

    replay = report.get("replay")
    if not isinstance(replay, dict):
        errors.append("missing replay object")
    else:
        for key in ("harness", "plan_sha256", "result_sha256"):
            if key not in replay:
                errors.append(f"replay missing '{key}'")

    environment = report.get("environment")
    if not isinstance(environment, dict):
        errors.append("missing environment object")

    return errors


def canonical_projection(report: dict[str, Any], max_protocols: int = 0) -> dict[str, Any]:
    runs_raw = report.get("runs")
    runs: list[dict[str, Any]] = runs_raw if isinstance(runs_raw, list) else []
    if max_protocols and max_protocols > 0:
        runs = runs[:max_protocols]

    config = report.get("config") if isinstance(report.get("config"), dict) else {}
    config_projection = {
        "mode": config.get("mode"),
        "solver": config.get("solver"),
        "depth": config.get("depth"),
        "k": config.get("k"),
        "timeout_secs": config.get("timeout_secs"),
        "samples": config.get("samples"),
        "soundness": config.get("soundness"),
        "fairness": config.get("fairness"),
        "require_pass": bool(config.get("require_pass")),
        "require_expectations": bool(config.get("require_expectations")),
    }

    return {
        "schema_version": report.get("schema_version"),
        "config": config_projection,
        "runs": [extract_run_replay_projection(run) for run in runs],
    }


def projection_hash(projection: dict[str, Any]) -> str:
    payload = json.dumps(projection, sort_keys=True, separators=(",", ":")).encode("utf-8")
    return hashlib.sha256(payload).hexdigest()


def collect_diffs(a: Any, b: Any, path: str = "") -> list[str]:
    if type(a) is not type(b):
        return [f"{path or '$'}: type mismatch ({type(a).__name__} vs {type(b).__name__})"]

    if isinstance(a, dict):
        diffs: list[str] = []
        a_keys = set(a.keys())
        b_keys = set(b.keys())
        for key in sorted(a_keys - b_keys):
            diffs.append(f"{path or '$'}: missing key in replay -> {key}")
        for key in sorted(b_keys - a_keys):
            diffs.append(f"{path or '$'}: extra key in replay -> {key}")
        for key in sorted(a_keys & b_keys):
            child = f"{path}.{key}" if path else key
            diffs.extend(collect_diffs(a[key], b[key], child))
        return diffs

    if isinstance(a, list):
        if len(a) != len(b):
            return [f"{path or '$'}: length mismatch ({len(a)} vs {len(b)})"]
        diffs: list[str] = []
        for idx, (lhs, rhs) in enumerate(zip(a, b)):
            child = f"{path}[{idx}]" if path else f"[{idx}]"
            diffs.extend(collect_diffs(lhs, rhs, child))
        return diffs

    if a != b:
        return [f"{path or '$'}: value mismatch ({a!r} vs {b!r})"]
    return []


def write_protocol_list(paths: list[str]) -> Path:
    tmp = tempfile.NamedTemporaryFile(prefix="tarsier-bench-replay-", suffix=".txt", delete=False)
    with tmp:
        for path in paths:
            tmp.write((path + "\n").encode("utf-8"))
    return Path(tmp.name)


def verify_protocol_hashes(runs: list[dict[str, Any]]) -> list[str]:
    errors: list[str] = []
    for idx, run in enumerate(runs):
        protocol = run.get("protocol")
        expected_hash = run.get("protocol_sha256")
        if not isinstance(protocol, str) or not protocol:
            errors.append(f"runs[{idx}]: invalid protocol path")
            continue
        if not isinstance(expected_hash, str) or len(expected_hash) != 64:
            errors.append(f"runs[{idx}]: invalid protocol_sha256")
            continue
        protocol_path = (ROOT / protocol).resolve()
        if not protocol_path.exists():
            errors.append(f"runs[{idx}]: protocol file missing: {protocol}")
            continue
        actual_hash = sha256_file(protocol_path)
        if actual_hash != expected_hash:
            errors.append(
                f"runs[{idx}]: protocol hash mismatch for {protocol} (expected {expected_hash}, got {actual_hash})"
            )
    return errors


def build_replay_command(
    baseline_report: dict[str, Any],
    protocol_list_file: Path,
    out_report: Path,
    skip_build: bool,
    max_protocols: int,
) -> list[str]:
    config = baseline_report["config"]
    cmd = [
        sys.executable,
        str((ROOT / "benchmarks" / "run_library_bench.py").resolve()),
        "--protocols",
        str(protocol_list_file),
        "--mode",
        str(config["mode"]),
        "--solver",
        str(config["solver"]),
        "--depth",
        str(config["depth"]),
        "--k",
        str(config["k"]),
        "--timeout",
        str(config["timeout_secs"]),
        "--samples",
        str(config["samples"]),
        "--soundness",
        str(config["soundness"]),
        "--fairness",
        str(config["fairness"]),
        "--out",
        str(out_report),
    ]
    if skip_build:
        cmd.append("--skip-build")
    if bool(config.get("require_pass")):
        cmd.append("--require-pass")
    if bool(config.get("require_expectations")):
        cmd.append("--require-expectations")
    if max_protocols and max_protocols > 0:
        cmd.extend(["--max-protocols", str(max_protocols)])
    return cmd


def main() -> int:
    args = parse_args()
    baseline_report_path = (ROOT / args.report).resolve()
    baseline_report = load_json(baseline_report_path)

    validation_errors = validate_report(baseline_report)
    if validation_errors:
        for err in validation_errors:
            print(f"[ERROR] {err}")
        return 2

    baseline_runs = baseline_report["runs"]
    if args.max_protocols and args.max_protocols > 0:
        baseline_runs = baseline_runs[: int(args.max_protocols)]

    hash_errors = verify_protocol_hashes(baseline_runs)
    if hash_errors:
        for err in hash_errors:
            print(f"[ERROR] {err}")
        return 2

    protocol_list_file = write_protocol_list([
        str(run["protocol"]) for run in baseline_runs
    ])

    tmp_dir = Path(tempfile.mkdtemp(prefix="tarsier-bench-replay-out-"))
    replay_report_path = (
        (ROOT / args.out_report).resolve()
        if args.out_report
        else tmp_dir / "replay-report.json"
    )
    replay_report_path.parent.mkdir(parents=True, exist_ok=True)

    cmd = build_replay_command(
        baseline_report=baseline_report,
        protocol_list_file=protocol_list_file,
        out_report=replay_report_path,
        skip_build=bool(args.skip_build),
        max_protocols=int(args.max_protocols) if args.max_protocols else 0,
    )

    proc = subprocess.run(
        cmd,
        cwd=str(ROOT),
        check=False,
        capture_output=True,
        text=True,
        encoding="utf-8",
    )
    if proc.returncode != 0:
        sys.stderr.write("Replay benchmark command failed.\n")
        if proc.stdout:
            sys.stderr.write(proc.stdout)
        if proc.stderr:
            sys.stderr.write(proc.stderr)
        return 2

    replay_report = load_json(replay_report_path)

    baseline_projection = canonical_projection(baseline_report, max_protocols=args.max_protocols)
    replay_projection = canonical_projection(replay_report, max_protocols=args.max_protocols)

    baseline_projection_hash = projection_hash(baseline_projection)
    replay_projection_hash = projection_hash(replay_projection)
    diffs = collect_diffs(baseline_projection, replay_projection)

    baseline_environment = baseline_report.get("environment", {})
    replay_environment = replay_report.get("environment", {})
    environment_match = baseline_environment == replay_environment
    if not environment_match and not args.allow_env_mismatch:
        diffs.insert(0, "environment metadata mismatch")

    baseline_plan = compute_replay_plan_sha256(
        copy.deepcopy(baseline_projection["config"]),
        baseline_projection["runs"],
    )
    replay_plan = compute_replay_plan_sha256(
        copy.deepcopy(replay_projection["config"]),
        replay_projection["runs"],
    )

    comparison = {
        "schema_version": 1,
        "source_report": str(baseline_report_path),
        "replay_report": str(replay_report_path),
        "checked_protocols": len(baseline_projection.get("runs", [])),
        "baseline_projection_sha256": baseline_projection_hash,
        "replay_projection_sha256": replay_projection_hash,
        "baseline_plan_sha256": baseline_plan,
        "replay_plan_sha256": replay_plan,
        "environment_match": environment_match,
        "mismatch_count": len(diffs),
        "mismatches": diffs[:50],
        "passed": not diffs,
    }

    comparison_path = (
        (ROOT / args.out_comparison).resolve()
        if args.out_comparison
        else replay_report_path.with_suffix(".comparison.json")
    )
    comparison_path.parent.mkdir(parents=True, exist_ok=True)
    comparison_path.write_text(json.dumps(comparison, indent=2), encoding="utf-8")

    if diffs:
        print("[FAIL] benchmark deterministic replay mismatch")
        print(f"  source={baseline_report_path}")
        print(f"  replay={replay_report_path}")
        for mismatch in diffs[:10]:
            print(f"  - {mismatch}")
        print(f"  comparison={comparison_path}")
        return 2

    print("[PASS] benchmark deterministic replay")
    print(f"  source={baseline_report_path}")
    print(f"  replay={replay_report_path}")
    print(f"  comparison={comparison_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
